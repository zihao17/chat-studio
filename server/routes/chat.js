/**
 * Chat API è·¯ç”±æ¨¡å—
 * æä¾› AI å¯¹è¯ä»£ç†æ¥å£ï¼Œæ”¯æŒé€šä¹‰åƒé—®å’Œ OpenAI æ¨¡å‹
 */

const express = require("express");
const OpenAI = require("openai");
const https = require("https");
const http = require("http");
const { getModelscopeApiKey } = require("../utils/keyManager");
const { getDatabase } = require("../db/database");
const router = express.Router();

/**
 * åˆ›å»ºè¿æ¥ä»£ç†ï¼ˆå¤ç”¨TCPè¿æ¥ï¼‰
 * @param {boolean} isHttps - æ˜¯å¦ä¸ºHTTPSè¿æ¥
 * @returns {Agent} - HTTP/HTTPSä»£ç†å®ä¾‹
 */
const createKeepAliveAgent = (isHttps = true) => {
  const Agent = isHttps ? https.Agent : http.Agent;
  return new Agent({
    keepAlive: true, // å¯ç”¨è¿æ¥å¤ç”¨
    keepAliveMsecs: 30000, // ç©ºé—²è¿æ¥ä¿æŒ30s
    maxSockets: 30, // æœ€å¤§å¹¶å‘è¿æ¥ï¼ˆæ ¹æ®APIå¹¶å‘é™åˆ¶è°ƒæ•´ï¼Œå¦‚30ï¼‰
    maxFreeSockets: 5, // ä¿ç•™5ä¸ªç©ºé—²è¿æ¥ï¼Œé¿å…é¢‘ç¹åˆ›å»º
    scheduling: "fifo", // å…ˆè¿›å…ˆå‡ºè°ƒåº¦ï¼Œé¿å…è¿æ¥é¥¥é¥¿
  });
};

/**
 * æ ¹æ®æ¨¡å‹åç§°åˆ¤æ–­ä½¿ç”¨å“ªä¸ª AI æœåŠ¡
 * @param {string} model - æ¨¡å‹åç§°
 * @returns {string} - æœåŠ¡ç±»å‹ï¼š'dashscope'ã€'modelscope' æˆ– 'openai'
 */
function getServiceType(model) {
  // é˜¿é‡Œç™¾ç‚¼é€šä¹‰åƒé—®æ¨¡å‹åˆ—è¡¨
  const dashscopeModels = [
    "qwen3-max",
    "qwen-flash-2025-07-28",
    "qwen3-vl-flash",
  ];

  // é­”æ­ModelScopeæ¨¡å‹åˆ—è¡¨
  const modelscopeModels = [
    "Qwen/Qwen3-Next-80B-A3B-Instruct",
    "ZhipuAI/GLM-4.6",
    "deepseek-ai/DeepSeek-R1-0528",
    "Qwen/Qwen3-235B-A22B",
    "Qwen/Qwen3-8B",
    "deepseek-ai/DeepSeek-V3.2-Exp",
  ];

  // OpenAI æ¨¡å‹åˆ—è¡¨
  const openaiModels = [];

  if (dashscopeModels.includes(model)) {
    return "dashscope";
  } else if (modelscopeModels.includes(model)) {
    return "modelscope";
  } else if (openaiModels.includes(model)) {
    return "openai";
  } else {
    // é»˜è®¤ä½¿ç”¨Qwen Next 80Bæ¨¡å‹
    return "modelscope";
  }
}

/**
 * åˆ›å»º OpenAI å®¢æˆ·ç«¯å®ä¾‹
 * @param {string} serviceType - æœåŠ¡ç±»å‹
 * @returns {OpenAI} - OpenAI å®¢æˆ·ç«¯å®ä¾‹
 */
function createOpenAIClient(serviceType) {
  if (serviceType === "dashscope") {
    return new OpenAI({
      apiKey: process.env.DASHSCOPE_API_KEY,
      baseURL: process.env.DASHSCOPE_BASE_URL,
      httpAgent: createKeepAliveAgent(false), // HTTPè¿æ¥å¤ç”¨
      httpsAgent: createKeepAliveAgent(true), // HTTPSè¿æ¥å¤ç”¨
      timeout: 60000, // å»¶é•¿è¶…æ—¶åˆ°60sï¼Œé€‚é…é•¿å“åº”
    });
  } else if (serviceType === "modelscope") {
    return new OpenAI({
      apiKey: getModelscopeApiKey(), // ä½¿ç”¨keyç®¡ç†å™¨è·å–å¯†é’¥
      baseURL: process.env.MODELSCOPE_BASE_URL,
      httpAgent: createKeepAliveAgent(false), // HTTPè¿æ¥å¤ç”¨
      httpsAgent: createKeepAliveAgent(true), // HTTPSè¿æ¥å¤ç”¨
      timeout: 60000, // å»¶é•¿è¶…æ—¶åˆ°60sï¼Œé€‚é…é•¿å“åº”
    });
  } else if (serviceType === "openai") {
    return new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
      baseURL: process.env.OPENAI_BASE_URL,
      httpAgent: createKeepAliveAgent(false), // HTTPè¿æ¥å¤ç”¨
      httpsAgent: createKeepAliveAgent(true), // HTTPSè¿æ¥å¤ç”¨
      timeout: 60000, // å»¶é•¿è¶…æ—¶åˆ°60sï¼Œé€‚é…é•¿å“åº”
    });
  } else {
    throw new Error(`ä¸æ”¯æŒçš„æœåŠ¡ç±»å‹: ${serviceType}`);
  }
}

/**
 * å‚æ•°æ ¡éªŒä¸­é—´ä»¶
 */
function validateChatRequest(req, res, next) {
  const { messages, model, temperature, max_tokens, top_p, user_system_prompt } = req.body;

  // æ ¡éªŒ messages å‚æ•°
  if (!messages || !Array.isArray(messages) || messages.length === 0) {
    return res.status(400).json({
      error: "Invalid request",
      message: "messages å‚æ•°å¿…é¡»æ˜¯éç©ºæ•°ç»„",
    });
  }

  // æ ¡éªŒæ¯ä¸ªæ¶ˆæ¯çš„æ ¼å¼
  for (const message of messages) {
    if (!message.role || !message.content) {
      return res.status(400).json({
        error: "Invalid request",
        message: "æ¯ä¸ªæ¶ˆæ¯å¿…é¡»åŒ…å« role å’Œ content å­—æ®µ",
      });
    }

    if (!["system", "user", "assistant"].includes(message.role)) {
      return res.status(400).json({
        error: "Invalid request",
        message: "role å­—æ®µå¿…é¡»æ˜¯ systemã€user æˆ– assistant",
      });
    }
  }

  // æ ¡éªŒ model å‚æ•°
  if (!model || typeof model !== "string") {
    return res.status(400).json({
      error: "Invalid request",
      message: "model å‚æ•°å¿…é¡»æ˜¯å­—ç¬¦ä¸²",
    });
  }

  // æ ¡éªŒ temperature å‚æ•°ï¼ˆå¯é€‰ï¼‰
  if (
    temperature !== undefined &&
    (typeof temperature !== "number" || temperature < 0 || temperature > 2)
  ) {
    return res.status(400).json({
      error: "Invalid request",
      message: "temperature å‚æ•°å¿…é¡»æ˜¯ 0-2 ä¹‹é—´çš„æ•°å­—",
    });
  }

  // æ ¡éªŒ max_tokens å‚æ•°ï¼ˆå¯é€‰ï¼‰
  if (
    max_tokens !== undefined &&
    (typeof max_tokens !== "number" || max_tokens < 1)
  ) {
    return res.status(400).json({
      error: "Invalid request",
      message: "max_tokens å‚æ•°å¿…é¡»æ˜¯å¤§äº 0 çš„æ•°å­—",
    });
  }

  // æ ¡éªŒ top_p å‚æ•°ï¼ˆå¯é€‰ï¼‰
  if (
    top_p !== undefined &&
    (typeof top_p !== "number" || top_p < 0 || top_p > 1)
  ) {
    return res.status(400).json({
      error: "Invalid request",
      message: "top_p å‚æ•°å¿…é¡»æ˜¯ 0-1 ä¹‹é—´çš„æ•°å­—",
    });
  }

  // æ ¡éªŒ user_system_prompt å‚æ•°ï¼ˆå¯é€‰ï¼‰
  if (
    user_system_prompt !== undefined &&
    typeof user_system_prompt !== "string"
  ) {
    return res.status(400).json({
      error: "Invalid request",
      message: "user_system_prompt å¿…é¡»æ˜¯å­—ç¬¦ä¸²",
    });
  }

  next();
}

/**
 * POST /api/chat - AI å¯¹è¯æ¥å£
 * æ”¯æŒæµå¼å’Œéæµå¼å“åº”
 */
router.post("/chat", validateChatRequest, async (req, res) => {
  const {
    messages,
    model,
    stream = true,
    temperature = 0.7,
    max_tokens = 10000,
    top_p = 0.9, // æ·»åŠ  top_p å‚æ•°æ”¯æŒï¼Œé»˜è®¤å€¼ 0.9
    user_system_prompt,
    // RAG ç›¸å…³å‚æ•°
    kb_enabled = false,
    kb_collection_id,
    kb_top_k = 6,
  } = req.body;

  // è®°å½•å¼€å§‹æ—¶é—´
  const startTime = Date.now();

  try {
    // åˆ¤æ–­ä½¿ç”¨å“ªä¸ª AI æœåŠ¡
    const serviceType = getServiceType(model);
    console.log(`ğŸ¤– ä½¿ç”¨ ${serviceType} æœåŠ¡ï¼Œæ¨¡å‹: ${model}`);

    // åˆ›å»ºå¯¹åº”çš„ OpenAI å®¢æˆ·ç«¯
    const openai = createOpenAIClient(serviceType);

    // å¹³å°ç³»ç»Ÿæç¤ºè¯ï¼ˆAï¼‰
    const platformSystemPrompt =
      "ä½ æ˜¯ Chat Studio æ™ºèƒ½ä¼™ä¼´ï¼Œä¸€ä¸ªä¸“ä¸ºå­¦ä¹ ã€å·¥ä½œä¸ç”Ÿæ´»åœºæ™¯è®¾è®¡çš„ AI åŠ©æ‰‹ï¼Œæ™ºèƒ½è´´å¿ƒä¸”å…¨é¢ã€‚è¯·ä»¥æ¸…æ™°ã€è‡ªç„¶ã€æœ‰å¸®åŠ©çš„æ–¹å¼å›åº”ç”¨æˆ·ã€‚å§‹ç»ˆä½¿ç”¨ç®€ä½“ä¸­æ–‡ï¼Œè¯­è¨€æµç•…ï¼Œè¯­æ°”å¯éšç”¨æˆ·é£æ ¼çµæ´»è°ƒæ•´ï¼ˆä¸“ä¸šä¸¥è°¨æˆ–è½»æ¾äº²åˆ‡ï¼‰ã€‚æ“…é•¿è§£ç­”çŸ¥è¯†é—®é¢˜ã€æä¾›å»ºè®®ã€è¾…åŠ©ï¼ˆå¦‚ä»£ç ã€æ–‡æœ¬ã€ç”Ÿæ´»ç­‰ï¼‰ï¼Œå¹¶èƒ½å¸®ç”¨æˆ·æ¢³ç†é€»è¾‘ã€è§£å†³å®é™…é—®é¢˜ã€‚";

    // ç”¨æˆ·è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯ï¼ˆBï¼‰
    const userSystemPrompt =
      user_system_prompt && typeof user_system_prompt === "string"
        ? user_system_prompt.trim()
        : "";

    // è½»é‡çº§å®‰å…¨åˆè§„æŒ‡ä»¤ï¼ˆSï¼‰
    const safetyPrompt =
      "è¯·éµå®ˆæ³•å¾‹æ³•è§„ä¸å¹³å°æ”¿ç­–ï¼Œä¸è¾“å‡ºè¿æ³•ã€éšç§æ³„éœ²ã€æ¶æ„åˆ©ç”¨æŒ‡å¼•ã€è¡€è…¥æš´åŠ›ã€ä»‡æ¨æ­§è§†æˆ–æˆäººéœ²éª¨å†…å®¹ã€‚æ¶‰åŠåŒ»ç–—ã€æ³•å¾‹ã€é‡‘èä¸å®‰å…¨ç­‰æ•æ„Ÿé¢†åŸŸï¼Œä»…æä¾›ä¸€èˆ¬æ€§ä¿¡æ¯å¹¶æç¤ºå’¨è¯¢ä¸“ä¸šäººå£«ï¼›å¦‚ä¸ç³»ç»ŸæŒ‡ä»¤æˆ–åˆè§„è¦æ±‚å†²çªï¼Œä»¥ç³»ç»ŸæŒ‡ä»¤ä¸åˆè§„è¦æ±‚ä¼˜å…ˆã€‚";

    // åˆæˆæœ€ç»ˆç³»ç»Ÿæç¤ºè¯ï¼ˆC = A + B + Sï¼‰
    const parts = [platformSystemPrompt];
    if (userSystemPrompt) parts.push(userSystemPrompt);
    parts.push(safetyPrompt);
    const finalSystemPrompt = parts.join("\n\n");

    // è¿‡æ»¤æ‰ç”¨æˆ·ä¼ æ¥çš„ system æ¶ˆæ¯ï¼Œç»Ÿä¸€ç”±æœåŠ¡ç«¯æ³¨å…¥ C
    const nonSystemMessages = Array.isArray(messages)
      ? messages.filter((m) => m && m.role !== "system")
      : [];

    const messagesWithSystem = [
      { role: "system", content: finalSystemPrompt },
      ...nonSystemMessages,
    ];

    // ï¼ˆæ—¥å¿—å·²ç§»é™¤ï¼‰

    // è®¾ç½®è¯·æ±‚è¶…æ—¶
    const timeout = parseInt(process.env.REQUEST_TIMEOUT) || 60000;
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), timeout);

    try {
      if (stream) {
        // æµå¼å“åº”
        res.setHeader("Content-Type", "text/event-stream");
        res.setHeader("Cache-Control", "no-cache");
        res.setHeader("Connection", "keep-alive");
        res.setHeader("Access-Control-Allow-Origin", "*");

        let totalTokens = 0;
        let promptTokens = 0;
        let completionTokens = 0;
        let citations = [];

        // è‹¥å¯ç”¨ RAGï¼šæ‰§è¡Œæ··æ£€+é‡æ’ï¼Œæ‹¼è£…ä¸Šä¸‹æ–‡
        let messagesFinal = messagesWithSystem;
        if (kb_enabled && kb_collection_id) {
          try {
            const { hybridSearch } = require("../utils/hybridSearch");
            const { rerank } = require("../utils/rerank");
            const collectionId = parseInt(kb_collection_id, 10);
            const userQuery = nonSystemMessages[nonSystemMessages.length - 1]?.content || "";
            const t0 = Date.now();
            const hybrid = await hybridSearch({ collectionId, query: userQuery, topK: 50 });
            // DashScope Rerank å¯¹è¾“å…¥æ–‡æ¡£å­˜åœ¨æ‰¹é‡ä¸Šé™ï¼ˆ<=10ï¼‰ã€‚
            const RERANK_INPUT_MAX = 10;
            const candidates = hybrid.slice(0, RERANK_INPUT_MAX);
            const docs = candidates.map((h) => h.content);
            let reranked;
            try {
              reranked = await rerank(userQuery, docs, Math.min(RERANK_INPUT_MAX, docs.length));
            } catch (err) {
              // å†ä¿é™©é™çº§ï¼šè‹¥ rerank å…¨é“¾è·¯å¤±è´¥ï¼Œåˆ™ä½¿ç”¨ hybrid æ’åºç›´æ¥å–å‰ kb_top_k
              console.warn("RERANK è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨ HYBRID ç›´æ¥æ’åºé™çº§:", err?.message || err);
              reranked = candidates.map((_, i) => ({ index: i, score: candidates[i].hybrid }));
            }
            const keep = reranked.slice(0, Math.min(kb_top_k, 6));
            // å–æ–‡æ¡£æ ‡é¢˜
            const docIds = Array.from(new Set(keep.map((r) => candidates[r.index].doc_id)));
            let titleMap = new Map();
            if (docIds.length) {
              const db = getDatabase();
              const placeholders = docIds.map(() => "?").join(",");
              const rows = await new Promise((resolve, reject) => {
                db.all(
                  `SELECT id, filename FROM kb_documents WHERE id IN (${placeholders})`,
                  docIds,
                  (err, rs) => (err ? reject(err) : resolve(rs || []))
                );
              });
              // å°è¯•ä¿®å¤å†å²ä¹±ç æ–‡ä»¶å
              const fixGarbled = (t) => {
                try {
                  const repaired = Buffer.from(String(t), 'latin1').toString('utf8');
                  const nonAsciiOrig = (String(t).match(/[^\x00-\x7F]/g) || []).length;
                  const nonAsciiNew = (repaired.match(/[^\x00-\x7F]/g) || []).length;
                  return nonAsciiNew > nonAsciiOrig ? repaired : t;
                } catch { return t; }
              };
              titleMap = new Map(rows.map((r) => [r.id, fixGarbled(r.filename)]));
            }
            citations = keep.map((r) => {
              const h = candidates[r.index];
              const title = titleMap.get(h.doc_id) || `doc-${h.doc_id}`;
              const preview = (h.content || "").slice(0, 120);
              return { title, preview, docId: h.doc_id, chunkId: h.chunk_id, score: r.score, idx: h.idx };
            });
            const contextBlocks = keep.map((r, i) => {
              const h = candidates[r.index];
              const title = citations[i]?.title || `doc-${h.doc_id}`;
              return `ã€æ¥æº${i + 1}ï½œ${title}ï½œ#${h.idx}ã€‘\n${h.content}`;
            });
            const ragPrefix = `ä½ å¯ä»¥å‚è€ƒä»¥ä¸‹å·²æ£€ç´¢åˆ°çš„èµ„æ–™ï¼ˆå¦‚æ— å…³è¯·å¿½ç•¥ï¼‰ï¼š\n\n${contextBlocks.join("\n\n")}`;
            messagesFinal = [
              { role: "system", content: finalSystemPrompt },
              { role: "system", content: ragPrefix },
              ...nonSystemMessages,
            ];
            console.log(`RAG æ£€ç´¢ç”¨æ—¶: ${(Date.now() - t0)}ms, å‘½ä¸­: ${keep.length}`);
          } catch (e) {
            console.warn("RAG æµç¨‹å¤±è´¥ï¼Œå›é€€æ— æ£€ç´¢:", e?.message || e);
          }
        }

        const completion = await openai.chat.completions.create(
          {
            model,
            messages: messagesFinal,
            stream: true,
            temperature,
            max_tokens,
            top_p, // æ·»åŠ  top_p å‚æ•°
            // æ·»åŠ  stream_options ä»¥è·å– token ç»Ÿè®¡ä¿¡æ¯
            stream_options: {
              include_usage: true,
            },
          },
          {
            signal: controller.signal,
          }
        );

        for await (const chunk of completion) {
          const content = chunk.choices[0]?.delta?.content || "";
          if (content) {
            // å‘é€ SSE æ ¼å¼æ•°æ®
            res.write(`data: ${JSON.stringify({ content })}\n\n`);
          }

          // æ”¶é›†tokenä½¿ç”¨ä¿¡æ¯
          if (chunk.usage) {
            totalTokens = chunk.usage.total_tokens || 0;
            promptTokens = chunk.usage.prompt_tokens || 0;
            completionTokens = chunk.usage.completion_tokens || 0;
          }
        }

        // è®¡ç®—å“åº”æ—¶é—´
        const responseTime = ((Date.now() - startTime) / 1000).toFixed(1);

        // å‘é€ç»Ÿè®¡ä¿¡æ¯
        res.write(
          `data: ${JSON.stringify({
            stats: {
              model,
              responseTime: `${responseTime}s`,
              totalTokens,
              promptTokens,
              completionTokens,
              citations,
            },
          })}\n\n`
        );

        res.write("data: [DONE]\n\n");
        res.end();
      } else {
        // éæµå¼å“åº”
        const completion = await openai.chat.completions.create(
          {
            model,
            messages: messagesWithSystem,
            stream: false,
            temperature,
            max_tokens,
            top_p, // æ·»åŠ  top_p å‚æ•°
          },
          {
            signal: controller.signal,
          }
        );

        const content = completion.choices[0]?.message?.content || "";

        // è®¡ç®—å“åº”æ—¶é—´
        const responseTime = ((Date.now() - startTime) / 1000).toFixed(1);

        res.json({
          content: content.trim(),
          model,
          usage: completion.usage,
          stats: {
            model,
            responseTime: `${responseTime}s`,
            totalTokens: completion.usage?.total_tokens || 0,
            promptTokens: completion.usage?.prompt_tokens || 0,
            completionTokens: completion.usage?.completion_tokens || 0,
          },
        });
      }
    } finally {
      clearTimeout(timeoutId);
    }
  } catch (error) {
    console.error("AI API è°ƒç”¨å¤±è´¥:", error);

    // å¤„ç†ä¸åŒç±»å‹çš„é”™è¯¯
    let statusCode = 500;
    let errorMessage = "AI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•";

    if (error.name === "AbortError") {
      statusCode = 408;
      errorMessage = "è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•";
    } else if (error.status) {
      // OpenAI API é”™è¯¯
      statusCode = error.status;
      if (error.status === 401) {
        errorMessage = "API å¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸ";
      } else if (error.status === 429) {
        errorMessage = "API è°ƒç”¨é¢‘ç‡è¶…é™ï¼Œè¯·ç¨åé‡è¯•";
      } else if (error.status === 400) {
        errorMessage = "è¯·æ±‚å‚æ•°é”™è¯¯";
      } else {
        errorMessage = error.message || errorMessage;
      }
    } else if (error.code === "ENOTFOUND" || error.code === "ECONNREFUSED") {
      statusCode = 503;
      errorMessage = "ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥";
    }

    // å¦‚æœæ˜¯æµå¼å“åº”ä¸”å·²ç»å¼€å§‹å‘é€æ•°æ®ï¼Œå‘é€é”™è¯¯äº‹ä»¶
    if (req.body.stream && res.headersSent) {
      res.write(`data: ${JSON.stringify({ error: errorMessage })}\n\n`);
      res.end();
    } else {
      res.status(statusCode).json({
        error: "API Error",
        message: errorMessage,
        code: error.code || "UNKNOWN_ERROR",
      });
    }
  }
});

module.exports = router;
