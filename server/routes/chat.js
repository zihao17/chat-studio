/**
 * Chat API è·¯ç”±æ¨¡å—
 * æä¾› AI å¯¹è¯ä»£ç†æ¥å£ï¼Œæ”¯æŒé€šä¹‰åƒé—®å’Œ OpenAI æ¨¡å‹
 */

const express = require("express");
const OpenAI = require("openai");
const router = express.Router();

/**
 * æ ¹æ®æ¨¡å‹åç§°åˆ¤æ–­ä½¿ç”¨å“ªä¸ª AI æœåŠ¡
 * @param {string} model - æ¨¡å‹åç§°
 * @returns {string} - æœåŠ¡ç±»å‹ï¼š'dashscope'ã€'modelscope' æˆ– 'openai'
 */
function getServiceType(model) {
  // é˜¿é‡Œç™¾ç‚¼é€šä¹‰åƒé—®æ¨¡å‹åˆ—è¡¨
  const dashscopeModels = [
    "qwen3-max",
    "qwen-flash-2025-07-28",
    "qwen3-vl-flash",
    "deepseek-v3.2-exp",
  ];

  // é­”æ­ModelScopeæ¨¡å‹åˆ—è¡¨
  const modelscopeModels = [
    "Qwen/Qwen3-Next-80B-A3B-Instruct",
    "ZhipuAI/GLM-4.5",
  ];

  // OpenAI æ¨¡å‹åˆ—è¡¨
  const openaiModels = [];

  if (dashscopeModels.includes(model)) {
    return "dashscope";
  } else if (modelscopeModels.includes(model)) {
    return "modelscope";
  } else if (openaiModels.includes(model)) {
    return "openai";
  } else {
    // é»˜è®¤ä½¿ç”¨é€šä¹‰åƒé—®
    return "dashscope";
  }
}

/**
 * åˆ›å»º OpenAI å®¢æˆ·ç«¯å®ä¾‹
 * @param {string} serviceType - æœåŠ¡ç±»å‹
 * @returns {OpenAI} - OpenAI å®¢æˆ·ç«¯å®ä¾‹
 */
function createOpenAIClient(serviceType) {
  if (serviceType === "dashscope") {
    return new OpenAI({
      apiKey: process.env.DASHSCOPE_API_KEY,
      baseURL: process.env.DASHSCOPE_BASE_URL,
    });
  } else if (serviceType === "modelscope") {
    return new OpenAI({
      apiKey: process.env.MODELSCOPE_API_KEY,
      baseURL: process.env.MODELSCOPE_BASE_URL,
    });
  } else if (serviceType === "openai") {
    return new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
      baseURL: process.env.OPENAI_BASE_URL,
    });
  } else {
    throw new Error(`ä¸æ”¯æŒçš„æœåŠ¡ç±»å‹: ${serviceType}`);
  }
}

/**
 * å‚æ•°æ ¡éªŒä¸­é—´ä»¶
 */
function validateChatRequest(req, res, next) {
  const { messages, model } = req.body;

  // æ ¡éªŒ messages å‚æ•°
  if (!messages || !Array.isArray(messages) || messages.length === 0) {
    return res.status(400).json({
      error: "Invalid request",
      message: "messages å‚æ•°å¿…é¡»æ˜¯éç©ºæ•°ç»„",
    });
  }

  // æ ¡éªŒæ¯ä¸ªæ¶ˆæ¯çš„æ ¼å¼
  for (const message of messages) {
    if (!message.role || !message.content) {
      return res.status(400).json({
        error: "Invalid request",
        message: "æ¯ä¸ªæ¶ˆæ¯å¿…é¡»åŒ…å« role å’Œ content å­—æ®µ",
      });
    }

    if (!["system", "user", "assistant"].includes(message.role)) {
      return res.status(400).json({
        error: "Invalid request",
        message: "role å­—æ®µå¿…é¡»æ˜¯ systemã€user æˆ– assistant",
      });
    }
  }

  // æ ¡éªŒ model å‚æ•°
  if (!model || typeof model !== "string") {
    return res.status(400).json({
      error: "Invalid request",
      message: "model å‚æ•°å¿…é¡»æ˜¯å­—ç¬¦ä¸²",
    });
  }

  next();
}

/**
 * POST /api/chat - AI å¯¹è¯æ¥å£
 * æ”¯æŒæµå¼å’Œéæµå¼å“åº”
 */
router.post("/chat", validateChatRequest, async (req, res) => {
  const {
    messages,
    model,
    stream = false,
    temperature = 0.7,
    max_tokens = 8000,
  } = req.body;

  // è®°å½•å¼€å§‹æ—¶é—´
  const startTime = Date.now();

  try {
    // åˆ¤æ–­ä½¿ç”¨å“ªä¸ª AI æœåŠ¡
    const serviceType = getServiceType(model);
    console.log(`ğŸ¤– ä½¿ç”¨ ${serviceType} æœåŠ¡ï¼Œæ¨¡å‹: ${model}`);

    // åˆ›å»ºå¯¹åº”çš„ OpenAI å®¢æˆ·ç«¯
    const openai = createOpenAIClient(serviceType);

    // ç¡®ä¿æœ‰ç³»ç»Ÿæ¶ˆæ¯
    const messagesWithSystem =
      messages[0]?.role === "system"
        ? messages
        : [
            {
              role: "system",
              content:
                "You are a helpful assistant. Please respond in Chinese.",
            },
            ...messages,
          ];

    // è®¾ç½®è¯·æ±‚è¶…æ—¶
    const timeout = parseInt(process.env.REQUEST_TIMEOUT) || 60000;
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), timeout);

    try {
      if (stream) {
        // æµå¼å“åº”
        res.setHeader("Content-Type", "text/event-stream");
        res.setHeader("Cache-Control", "no-cache");
        res.setHeader("Connection", "keep-alive");
        res.setHeader("Access-Control-Allow-Origin", "*");

        let totalTokens = 0;
        let promptTokens = 0;
        let completionTokens = 0;

        const completion = await openai.chat.completions.create(
          {
            model,
            messages: messagesWithSystem,
            stream: true,
            temperature,
            max_tokens,
            // æ·»åŠ  stream_options ä»¥è·å– token ç»Ÿè®¡ä¿¡æ¯
            stream_options: {
              include_usage: true
            }
          },
          {
            signal: controller.signal,
          }
        );

        for await (const chunk of completion) {
          const content = chunk.choices[0]?.delta?.content || "";
          if (content) {
            // å‘é€ SSE æ ¼å¼æ•°æ®
            res.write(`data: ${JSON.stringify({ content })}\n\n`);
          }
          
          // æ”¶é›†tokenä½¿ç”¨ä¿¡æ¯
          if (chunk.usage) {
            totalTokens = chunk.usage.total_tokens || 0;
            promptTokens = chunk.usage.prompt_tokens || 0;
            completionTokens = chunk.usage.completion_tokens || 0;
          }
        }

        // è®¡ç®—å“åº”æ—¶é—´
        const responseTime = ((Date.now() - startTime) / 1000).toFixed(1);

        // å‘é€ç»Ÿè®¡ä¿¡æ¯
        res.write(`data: ${JSON.stringify({ 
          stats: {
            model,
            responseTime: `${responseTime}s`,
            totalTokens,
            promptTokens,
            completionTokens
          }
        })}\n\n`);

        res.write("data: [DONE]\n\n");
        res.end();
      } else {
        // éæµå¼å“åº”
        const completion = await openai.chat.completions.create(
          {
            model,
            messages: messagesWithSystem,
            stream: false,
            temperature,
            max_tokens,
          },
          {
            signal: controller.signal,
          }
        );

        const content = completion.choices[0]?.message?.content || "";
        
        // è®¡ç®—å“åº”æ—¶é—´
        const responseTime = ((Date.now() - startTime) / 1000).toFixed(1);

        res.json({
          content: content.trim(),
          model,
          usage: completion.usage,
          stats: {
            model,
            responseTime: `${responseTime}s`,
            totalTokens: completion.usage?.total_tokens || 0,
            promptTokens: completion.usage?.prompt_tokens || 0,
            completionTokens: completion.usage?.completion_tokens || 0
          }
        });
      }
    } finally {
      clearTimeout(timeoutId);
    }
  } catch (error) {
    console.error("AI API è°ƒç”¨å¤±è´¥:", error);

    // å¤„ç†ä¸åŒç±»å‹çš„é”™è¯¯
    let statusCode = 500;
    let errorMessage = "AI æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•";

    if (error.name === "AbortError") {
      statusCode = 408;
      errorMessage = "è¯·æ±‚è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•";
    } else if (error.status) {
      // OpenAI API é”™è¯¯
      statusCode = error.status;
      if (error.status === 401) {
        errorMessage = "API å¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸ";
      } else if (error.status === 429) {
        errorMessage = "API è°ƒç”¨é¢‘ç‡è¶…é™ï¼Œè¯·ç¨åé‡è¯•";
      } else if (error.status === 400) {
        errorMessage = "è¯·æ±‚å‚æ•°é”™è¯¯";
      } else {
        errorMessage = error.message || errorMessage;
      }
    } else if (error.code === "ENOTFOUND" || error.code === "ECONNREFUSED") {
      statusCode = 503;
      errorMessage = "ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥";
    }

    // å¦‚æœæ˜¯æµå¼å“åº”ä¸”å·²ç»å¼€å§‹å‘é€æ•°æ®ï¼Œå‘é€é”™è¯¯äº‹ä»¶
    if (req.body.stream && res.headersSent) {
      res.write(`data: ${JSON.stringify({ error: errorMessage })}\n\n`);
      res.end();
    } else {
      res.status(statusCode).json({
        error: "API Error",
        message: errorMessage,
        code: error.code || "UNKNOWN_ERROR",
      });
    }
  }
});

module.exports = router;
